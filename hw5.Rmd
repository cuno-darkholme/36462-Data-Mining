---
title: "hw5"
author: "Jin Chen"
date: "2/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Code from lecture 10**

```{r}
nyt.frame <- read.csv("http://www.stat.cmu.edu/~cshalizi/dm/20/lectures/10/nyt.frame.csv")
class.labels <- c(rep("art", 57), rep("music", 45))
nyt.frame <- data.frame(class.labels = as.factor(class.labels), nyt.frame)
```

Entropy function

```{r}
# Calculate the entropy of a vector of counts or proportions Inputs: Vector of
# numbers Output: Entropy (in bits)
entropy <- function(p) {
# Assumes: p is a numeric vector
if (sum(p) == 0) {
return(0) # Case shows up when calculating conditional
# entropies
}
p <- p/sum(p) # Normalize so it sums to 1
p <- p[p > 0] # Discard zero entries (because 0 log 0 = 0)
H <- -sum(p * log(p, base = 2))
return(H)
}
```

Mutual information function

```{r}
# Get the expected information a word's indicator gives about a document's class
# Inputs: array of indicator counts Calls: entropy() Outputs: mutual information
word.mutual.info <- function(counts) {
# Assumes: counts is a numeric matrix get the marginal entropy of the classes
# (rows) C
marginal.entropy = entropy(rowSums(counts))
# Get the probability of each value of X
probs <- colSums(counts)/sum(counts)
# Calculate the entropy of each column
column.entropies = apply(counts, 2, entropy)
conditional.entropy = sum(probs * column.entropies)
mutual.information = marginal.entropy - conditional.entropy
return(mutual.information)
}
```

Class indicator count function

```{r}
# Count how many documents in each class do or don't contain a word Presumes that
# the data frame contains a column, named 'class.labels', which has the classes
# labels; may be more than 2 classes Inputs: dataframe of word counts with class
# labels (BoW), word to check (word) Outputs: table of counts
word.class.indicator.counts <- function(BoW, word) {
# What are the classes?
classes <- levels(BoW[, "class.labels"])
# Prepare a matrix to store the counts, 1 row per class, 2 cols (for
# present/absent)
counts <- matrix(0, nrow = length(classes), ncol = 2)
# Name the rows to match the classes
rownames(counts) = classes
for (i in 1:length(classes)) {
# Get a Boolean vector showing which rows belong to the class
instance.rows = (BoW[, "class.labels"] == classes[i])
# sum of a boolean vector is the number of TRUEs
n.class = sum(instance.rows) # Number of class instances
present = sum(BoW[instance.rows, word] > 0)
# present = Number of instances of class containing the word
counts[i, 1] = present
counts[i, 2] = n.class - present
}
return(counts)
}
```

Info Bows function

```{r}
# Calculate realized and expected information of word indicators for classes
# Assumes: one column of the data is named 'class.labels' Inputs: data frame of
# word counts with class labels Calls: word.class.indicator.counts(),
# word.realized.info(), word.mutual.info() Output: two-column matrix giving the
# reduction in class entropy when a word is present, and the expected reduction
# from checking the word
info.bows <- function(BoW) {
lexicon <- colnames(BoW)
# One of these columns will be class.labels, that's not a lexical item
lexicon <- setdiff(lexicon, "class.labels")
vocab.size = length(lexicon)
word.infos <- matrix(0, nrow = vocab.size, ncol = 2)
# Name the rows so we know what we're talking about
rownames(word.infos) = lexicon
for (i in 1:vocab.size) {
counts <- word.class.indicator.counts(BoW, lexicon[i])
word.infos[i, 1] = word.mutual.info(counts)
word.infos[i, 2] = word.mutual.info(counts)
}
return(word.infos)
}
```

```{r}
nyt.indicators = data.frame(class.labels = nyt.frame[, 1], nyt.frame[, -1] > 0)
```


Columns to Table function

```{r}
# Create a multi-dimensional table from given columns of a data-frame Inputs:
# frame, vector of column numbers or names Outputs: multidimensional contingency
# table
columns.to.table <- function(frame, colnums) {
my.factors = c()
for (i in colnums) {
# Create commands to pick out individual columns, but don't evaluate them yet
my.factors = c(my.factors, substitute(frame[, i], list(i = i)))
}
# paste those commands together
col.string = paste(my.factors, collapse = ", ")
# Name the dimensions of the table for comprehensibility
if (is.numeric(colnums)) {
# if we gave column numbers, get names from the frame
table.names = colnames(frame)[colnums]
} else {
# if we gave column names, use them
table.names = colnums
}
# Encase the column names in quotation marks to make sure they stay names and R
# doesn't try to evaluate them
table.string = paste("\"", table.names, "\"", collapse = ",")
# paste them together
table.string = paste("c(", table.string, ")", collapse = ",")
# Assemble what we wish we could type at the command line
expr = paste("table(", col.string, ", dnn=", table.string, ")", collapse = "")
# execute it parse() takes a string and parses it but doesn't evaluate it eval()
# actually substitutes in values and executes commands
return(eval(parse(text = expr)))
}
```

Joint entropy

```{r}
jt.entropy.columns = function(frame, colnums) {
tabulations = columns.to.table(frame, colnums)
H = entropy(as.vector(tabulations))
return(H)
}
```

```{r}
jt.entropy.columns(nyt.indicators, c("art", "painting", "evening"))
```

Info in multi columns

```{r}
# Compute the information in multiple features about the outcome Inputs: data
# frame, vector of feature numbers, number of target feature (optional,
# default=1) Calls: jt.entropy.columns Output: mutual information in bits
info.in.multi.columns = function(frame, feature.cols, target.col = 1) {
H.target = jt.entropy.columns(frame, target.col)
H.features = jt.entropy.columns(frame, feature.cols)
H.joint = jt.entropy.columns(frame, c(target.col, feature.cols))
return(H.target + H.features - H.joint)
}
```

Info in extra column

```{r}
# Information about target after adding a new column to existing set Inputs: new
# column, vector of old columns, data frame, target column (default 1) Calls:
# info.in.multi.columns() Output: new mutual information, in bits
info.in.extra.column <- function(new.col, old.cols, frame, target.col = 1) {
mi = info.in.multi.columns(frame, c(old.cols, new.col), target.col = target.col)
return(mi)
}

```

Best next column

```{r}
# Identify the best column to add to an existing set Inputs: data frame,
# currently-picked columns, target column (default 1) Calls:
# info.in.extra.column() Output: index of the best feature
best.next.column <- function(frame, old.cols, target.col = 1) {
# Which columns might we add?
possible.cols = setdiff(1:ncol(frame), c(old.cols, target.col))
# How good are each of those columns?
infos = sapply(possible.cols, info.in.extra.column, old.cols = old.cols, frame = frame,
target.col = target.col)
# which of these columns is biggest?
best.possibility = which.max(infos)
# what column of the original data frame is that?
best.index = possible.cols[best.possibility]
return(best.index)
}
```

Best q columns

```{r}
# Identify the best q columns for a given target variable Inputs: data frame, q,
# target column (default 1) Calls: best.next.column() Output: vector of column
# indices
best.q.columns <- function(frame, q, target.col = 1) {
possible.cols = setdiff(1:ncol(frame), target.col)
selected.cols = c()
for (k in 1:q) {
new.col = best.next.column(frame, selected.cols, target.col)
selected.cols = c(selected.cols, new.col)
}
return(selected.cols)
}

```

**Question 2:**

Consider classifying images using their color content. That is, we represent an image by a vector which says, for each color, how many pixels there were of that color. (This is sometimes called a “bag-of-colors” representation.) There are two classes. Our two favorite colors are x and y; the ﬁgures below show the distribution of pixel-counts for x (on the left) and y (on the right), with solid or dashed lines indicating the diﬀerent classes.

![A local image](1.png)/

Which color gives us more information about the image’s class? Why?

Answer: The color y gives us more information about the image's class. For color y, the two classes are more separated, which mean we could set a decision boundary to correctly classify most of the data points.Thus color y is more informative to classify those two classes.

**Question 3:**

Explain how a feature can provide information that lets us discriminate between classes, even though it has the same average value in each class. (You may want to draw some histograms.)

```{r}
df3 <- data.frame("x" = c(1,3.8,3.9,4.1,4.2,4.9,5,5.1), "y" = c(1,-1,-1,-1,-1,1,1,1))
plot(df3$x,df3$y)
```

In the dataframe above, feature x has the same average value x=4 in the two classes y=-1 and y=1. But we could still use x to distingush those two classes. For example,if we set x=4.5 as the decision boundary, most of the points will be classifed correctly, thus x is informative in classifying y.

**Question 4:**
Consider the following cross-tabulation table between two discrete variables, X and Y .

![A local image](2.png)/

There is (at least) one function b of X such that I[Y ;b(X)] = I[Y ;X] but H[b(X)] < H[X]. That is, there is a way to compress X which loses no predictive information about Y . Find such a function b. Can you describe it in words?

Answer: The function b I found is: if x=1 or x=3 or x=5, b(x)=1; if x=2 or x=4 or x=6, b(x)=-1. H[b(x)] is less than H(x) since there are less classes for x. The I[Y;b(x)] is equal to H[Y;X] since for x=1,x=3 and x=5, the distribution for y are both 50/50. This is also the same for x=2,x=4 and x=6. So combining those x won't lose any predictive information about Y.

```{r}
entropy(c(26,100,84,80,24,130))
entropy(c(134,310))
```

H[b(x)]=0.88348 is less than H[x]=2.37

**Question 5:**
There are two types of widgets, foos and bars. Some widgets contain baz and some do not. Consider the following contingency table.

![A local image](3.png)/

(a) How many widgets are foos? How many are bars? How many contain baz? What is the probability that a random widget is foo? What is the probability that a random widget contains baz? 

The number of foos: 7+611=618

The number of bars: 250+694=944

The number containing baz: 611+694=1305

The probability of foo: 618/(618+944) = 0.3956

The probability of containing baz: 1305/(618+944) = 0.8355

(b) What is the entropy of widget type? 

$H(x) = -\sum_{x}{p(x)*log_{2}{p(x)}} =- (0.3956 * log_{2}{0.3956} + 0.6044 * log_{2}{0.6044}) = 0.9683$

There are 618 foos and 944 bars.

```{r}
entropy(c(618,944))
```


(c) What is the entropy of widget type conditional on whether or not baz is present? 

$H[Y|X] = \sum_{x}{p(x)*H[Y|X=x]} = 0.8355 * H[Y|X=baz] + 0.1645 * H[Y|X=absent] = 0.8355*0.9971 + 0.1645 * 0.1803 = 0.8627$

Two helper calculations.

```{r}
entropy(c(611,694))
entropy(c(7,250))
```

The conditional entropy is 0.8627

(d) What is the mutual information between widget type and whether or not baz is present?

```{r}
word.mutual.info(matrix(c(7,250,611,694),nrow=2))
```

The mutual information is 0.105647

**Question 6:**
Refer to the handout for lecture 10, where the greedy feature-selection is used to pick the seven most informative words in the Times corpus.

(a) How much information does each of those words provide about the class, given the other six words? 

```{r}
best7 = best.q.columns(nyt.indicators,7)
```

```{r}

info_6a <- c()
for (i in 1:7){
  best1 <- best7[i]
  best6 = best7[best7 != best1]
  info = info.in.extra.column(best1,best6,nyt.indicators,target.col = 1)
  info_6a[i] <- info - info.in.multi.columns(nyt.indicators,best6)
}

data.frame(as.table(setNames(info_6a,best7)))
```

Table above shows the mutual information of those 7 words given other 6.

(b) Which words (if any) have positive interactions with the other six, and which (if any) have negative interactions? 

```{r}
hc <- entropy(c(57,45))
info_6b <- c()
for (i in 1:7){
  best1 <- best7[i]
  info_1 <- info.in.multi.columns(nyt.indicators,best1)
  info_change <- hc - info_1
  info_6b[i] <- info_change
}

info_change <- info_6a - info_6b
data.frame(as.table(setNames(info_change,best7)))
```

We can see from the table above that all I[C;Y|X]-I[c;Y] is less than zero. Which means all 7 words have negetive interaction.

(c) Use CART and 1-nearest-neighbor classiﬁcation on all the features; what is the mis-classiﬁcation rate, under 10-fold cross-validation? 

```{r,include=FALSE}
library(tree)
```

```{r}
set.seed(114514)
new_tree <- tree(class.labels ~ .,data = nyt.frame)
cv_tree <- cv.tree(new_tree,FUN = prune.tree,K = 10,method = "misclass")
```

```{r}
cv_tree
plot(cv_tree)
```

As shown above, the misclass in the max_tree is 18, so the accuracy is (102-18)/102=0.8235

```{r, include=FALSE}
library(caret)
```

```{r}
Control <- trainControl(method = "repeatedcv", number = 10)
knn <- train(class.labels~.,
             method = "knn",
             tuneGrid = expand.grid(k = 1),
             trControl = Control,
             data = nyt.frame)
knn
```

As shown above, the accuracy of KNN when K=1 under 10-fold cv is 0.6572727.

(d) What are the misclassiﬁcation rates of CART and the nearestneighbor classiﬁer using just the seven selected features? (Again, report results under 10-fold CV.)

```{r}
set.seed(114514)
new_tree2 <- tree(class.labels ~ art+youre+features+music+gallery+heavy+second,data = nyt.frame)
cv_tree2 <- cv.tree(new_tree2,FUN = prune.tree,K = 10,method = "misclass")
```

```{r}
cv_tree2
plot(cv_tree2)
```

The accuracy remained unchanged (0.8235) when using the maximum size. But I noticed that when using lower sizes, the accuracy is higher than the tree in 6.c

```{r}
Control <- trainControl(method = "repeatedcv", number = 10)
knn2 <- train(class.labels~art+youre+features+music+gallery+heavy+second,
             method = "knn",
             tuneGrid = expand.grid(k = 1),
             trControl = Control,
             data = nyt.frame)
knn2
```

The accuracy is 0.9264646 now, which has greatly increased.

**Question 7:**

The code for lecture 10 contains a function to pick the q most informative features in a data-frame by greedy search.

(a) How much information does the trio of words (“art”, “painting”, “evening”) give about the story class? 

Get the column index.

```{r}
which(colnames(nyt.indicators)=="art")
which(colnames(nyt.indicators)=="painting")
which(colnames(nyt.indicators)=="evening")
```

```{r}
q7a<-c(244,2270,1328)
info.in.multi.columns(nyt.indicators,q7a)
```

The info we can get from the 3 columns is 0.4104331

(b) Write a function which will pick the q most informative features to add to a given set of starting features. The function should take as inputs: a data frame, the column in the data frame which is to be predicted, the vector of features to start from, and q, the number of features to add. For full credit, the user should be able to specify features either by column numbers or column names. Test your function by checking that it gives the same results as in Lecture 10 when started with “art” and (“art”, “youre”), and q = 1 or q = 2. 

Helper functions wich convert the column names to column numbers

```{r}
# convert column names to column numbers
converter <- function(frame,vector){
  new_vector <- c()
  for (j in 1:length(vector)){
    if (is.na(as.numeric(vector[j])) == TRUE){
    new_vector[j] <- which(colnames(frame)==vector[j])
  } else{
    new_vector[j] <- vector[j]
  }
  }
  return(new_vector)
}
```

main function

```{r}
pick.q.most <- function(frame, prior.col, q, target.col){
  target <- converter(frame,target.col)
  prior <- converter(frame,prior.col)
  possible.cols = setdiff(1:ncol(frame), c(target,prior))
  selected.cols = prior
  for (k in 1:q) {
    new.col = best.next.column(frame, selected.cols, target)
    selected.cols = c(selected.cols, new.col)
  }
  return(setdiff(selected.cols,prior))
}
```

Testing

```{r,warning = FALSE}
pick.q.most(nyt.indicators,c("art","youre"),1,1)
pick.q.most(nyt.indicators,244,2,1)
```

(c) What are the three most informative words to add to (“art”, “painting”, “evening”)? How much information do they add? How much information do they provide on their own?

```{r,include=FALSE}
which(colnames(nyt.indicators)=="art")
which(colnames(nyt.indicators)=="painting")
which(colnames(nyt.indicators)=="evening")
```

```{r,echo=FALSE,warning=FALSE}
q3 <- c("art","painting","evening")
picked <- pick.q.most(nyt.indicators,q3,3,1)
```

```{r}
picked_columns <- c()
for (i in 1:3){
  picked_columns[i] <- colnames(nyt.indicators[picked[i]])
}
picked_columns
```

As shown above, the three words to be added are "youre", "events" and "work".

```{r,warning=FALSE}
q6 <- c(q3,picked_columns)
info.in.multi.columns(nyt.indicators,converter(nyt.indicators,q6))-info.in.multi.columns(nyt.indicators,converter(nyt.indicators,q3))
```

As shown, the information they add is 0.3420021.

```{r}
info.in.multi.columns(nyt.indicators,converter(nyt.indicators,picked))
```

The information they provide in their own is 0.2405166.

```{r}
info.in.multi.columns(nyt.indicators,4429)
info.in.multi.columns(nyt.indicators,1331)
info.in.multi.columns(nyt.indicators,4382)
```

The information each of those words provided are: "youre" 0.0729; "events" 0.06147; "work" 0.10421.

We can see that the information they add are more than their own information.